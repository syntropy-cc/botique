#!/usr/bin/env python3
"""
Script de produ√ß√£o para gerar ideias de posts a partir de um artigo.

Usa o workflow completo de produ√ß√£o com logging integrado.
"""

import json
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

from src.core.config import IdeationConfig, OUTPUT_DIR
from src.core.llm_client import HttpLLMClient
from src.core.llm_logger import LLMLogger
from src.core.prompt_registry import get_latest_prompt
from src.ideas.generator import IdeaGenerator

# Carregar vari√°veis de ambiente
load_dotenv()

def main():
    print("=" * 70)
    print("GERA√á√ÉO DE IDEIAS - PRODU√á√ÉO")
    print("=" * 70)
    
    # Caminhos
    article_path = Path("articles/why-tradicional-learning-fails.md")
    
    if not article_path.exists():
        print(f"‚ùå ERRO: Artigo n√£o encontrado: {article_path}")
        return 1
    
    print(f"‚úì Artigo encontrado: {article_path}")
    
    # Verificar API key
    api_key = os.getenv("LLM_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("‚ùå ERRO: LLM_API_KEY ou DEEPSEEK_API_KEY n√£o encontrada no .env")
        return 1
    
    print(f"‚úì API Key encontrada: {api_key[:10]}...")
    
    # Ler artigo
    print(f"\n1. Lendo artigo...")
    article_text = article_path.read_text(encoding="utf-8")
    article_slug = article_path.stem
    print(f"   ‚úì Artigo lido: {len(article_text)} caracteres")
    print(f"   ‚úì Article slug: {article_slug}")
    
    # Criar logger with SQL backend
    print(f"\n2. Inicializando logger...")
    logger = LLMLogger(use_sql=True)
    logger.set_context(article_slug=article_slug)
    
    # Create trace for this execution
    trace_id = logger.create_trace(
        name="generate_ideas",
        metadata={"article_slug": article_slug, "article_path": str(article_path)},
    )
    print(f"   ‚úì Logger criado: session_id={logger.get_session_id()[:8]}...")
    print(f"   ‚úì Trace criado: trace_id={trace_id[:8]}...")
    print(f"   ‚úì SQL logging: habilitado")
    
    # Criar diret√≥rio de output primeiro
    output_dir = OUTPUT_DIR / article_slug
    output_dir.mkdir(parents=True, exist_ok=True)
    debug_dir = output_dir / "debug"
    
    # Criar cliente LLM com logger (timeout aumentado para respostas grandes)
    # Configurar para salvar respostas brutas automaticamente
    print(f"\n3. Criando cliente LLM...")
    llm_client = HttpLLMClient(
        api_key=api_key,
        base_url="https://api.deepseek.com/v1",
        model="deepseek-chat",
        timeout=180,  # 3 minutos para respostas grandes
        logger=logger,
        save_raw_responses=True,  # Habilitar salvamento autom√°tico
        raw_responses_dir=debug_dir,  # Diret√≥rio espec√≠fico para este artigo
    )
    print(f"   ‚úì Cliente LLM criado: model={llm_client.model}, timeout={llm_client.timeout}s")
    print(f"   ‚úì Salvamento autom√°tico de respostas brutas: habilitado")
    
    # Verificar se o prompt est√° registrado no banco de dados
    print(f"\n4. Verificando prompt no banco de dados...")
    prompt_key = "post_ideator"
    prompt_data = get_latest_prompt(prompt_key)
    if not prompt_data:
        print(f"   ‚ùå ERRO: Prompt '{prompt_key}' n√£o encontrado no banco de dados!")
        print(f"   üìù Por favor, registre o prompt primeiro usando:")
        print(f"      python -m src.cli.commands prompts register prompts/post_ideator.md")
        print(f"   ou use o script de registro de prompts.")
        return 1
    
    prompt_version = prompt_data.get("version", "N/A")
    print(f"   ‚úì Prompt encontrado: {prompt_key} (vers√£o {prompt_version})")
    
    # Criar gerador de ideias
    print(f"\n5. Criando gerador de ideias...")
    generator = IdeaGenerator(llm_client)
    print(f"   ‚úì Gerador criado")
    
    # Configurar ideation
    print(f"\n6. Configurando par√¢metros de idea√ß√£o...")
    config = IdeationConfig(
        num_ideas_min=5,
        num_ideas_max=8,
        num_insights_min=3,
        num_insights_max=5,
    )
    print(f"   ‚úì Configura√ß√£o:")
    print(f"     - Ideias: {config.num_ideas_min}-{config.num_ideas_max}")
    print(f"     - Insights: {config.num_insights_min}-{config.num_insights_max}")
    
    # Gerar ideias (resposta bruta ser√° salva automaticamente pelo HttpLLMClient)
    print(f"\n7. Gerando ideias (chamada real ao LLM)...")
    print(f"   ‚è≥ Isso pode levar alguns segundos...")
    print(f"   üìù A resposta bruta ser√° salva automaticamente em: {debug_dir}")
    
    try:
        payload = generator.generate_ideas(
            article_text,
            config,
            context=article_slug,  # Contexto para organizar respostas salvas
        )
        print(f"   ‚úì Ideias geradas com sucesso!")
        
        # Verificar se a resposta bruta foi salva
        raw_files = list(debug_dir.glob("raw_response_*.txt"))
        if raw_files:
            latest_raw = max(raw_files, key=lambda p: p.stat().st_mtime)
            print(f"   ‚úì Resposta bruta salva automaticamente: {latest_raw.name}")
        
    except Exception as e:
        print(f"   ‚ùå Erro ao gerar ideias: {e}")
        import traceback
        traceback.print_exc()
        
        # Verificar se a resposta bruta foi salva mesmo com erro
        raw_files = list(debug_dir.glob("raw_response_*.txt"))
        if raw_files:
            latest_raw = max(raw_files, key=lambda p: p.stat().st_mtime)
            print(f"\n   ‚úì Resposta bruta salva mesmo com erro: {latest_raw.name}")
        
        return 1
    
    # Validar resultado
    ideas = payload.get("ideas", [])
    article_summary = payload.get("article_summary", {})
    
    print(f"\n8. Resultados:")
    print(f"   ‚úì Total de ideias geradas: {len(ideas)}")
    print(f"   ‚úì Insights identificados: {len(article_summary.get('key_insights', []))}")
    print(f"   ‚úì T√≠tulo do artigo: {article_summary.get('title', 'N/A')}")
    
    # Salvar JSON
    output_path = output_dir / "phase1_ideas.json"
    print(f"\n9. Salvando resultados...")
    output_path.write_text(
        json.dumps(payload, indent=2, ensure_ascii=False),
        encoding="utf-8",
    )
    print(f"   ‚úì JSON salvo: {output_path}")
    
    # Resumo das ideias
    print(f"\n10. Resumo das ideias geradas:")
    for idx, idea in enumerate(ideas, 1):
        print(f"   {idx}. {idea.get('id', 'N/A')}: {idea.get('platform', 'N/A')} - {idea.get('tone', 'N/A')}")
        print(f"      Hook: {idea.get('hook', 'N/A')[:60]}...")
        print(f"      Slides estimados: {idea.get('estimated_slides', 'N/A')}")
        print(f"      Confian√ßa: {idea.get('confidence', 0):.2f}")
    
    # Resumo de uso LLM
    if logger.calls:
        total_calls = len(logger.calls)
        total_tokens = sum(
            call["metrics"]["tokens_total"]
            for call in logger.calls
            if call["metrics"]["tokens_total"] is not None
        )
        total_cost = sum(
            call["metrics"]["cost_estimate"]
            for call in logger.calls
            if call["metrics"]["cost_estimate"] is not None
        )
        
        print(f"\n11. Uso LLM:")
        print(f"   ‚úì Total de chamadas: {total_calls}")
        if total_tokens:
            print(f"   ‚úì Total de tokens: {total_tokens:,}")
        if total_cost:
            print(f"   ‚úì Custo estimado: ${total_cost:.6f}")
    
    # Verify SQL database
    print(f"\n12. Verificando banco de dados SQL...")
    try:
        from src.core.llm_log_queries import get_trace_with_events, get_cost_summary
        from src.core.llm_log_db import get_db_path
        
        db_path = get_db_path()
        trace_data = get_trace_with_events(trace_id, db_path)
        
        if trace_data:
            event_count = len(trace_data.get("events", []))
            print(f"   ‚úì Trace encontrado no banco: {trace_id[:8]}...")
            print(f"   ‚úì Eventos salvos: {event_count}")
            
            # Get cost summary for this trace
            cost_summary = get_cost_summary(
                filters={"trace_id": trace_id},
                db_path=db_path,
            )
            if cost_summary["summary"]["total_cost"]:
                print(f"   ‚úì Custo total (do banco): ${cost_summary['summary']['total_cost']:.6f}")
        else:
            print(f"   ‚ö†Ô∏è  Trace n√£o encontrado no banco")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erro ao verificar banco: {e}")
    
    print("\n" + "=" * 70)
    print("‚úÖ GERA√á√ÉO CONCLU√çDA COM SUCESSO!")
    print("=" * 70)
    print(f"\nüìÑ Arquivo de ideias: {output_path}")
    print(f"üìä Trace ID: {trace_id}")
    
    return 0

if __name__ == "__main__":
    exit(main())

